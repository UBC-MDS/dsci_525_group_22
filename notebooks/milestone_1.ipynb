{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import zipfile\n",
    "import glob\n",
    "import re\n",
    "import pyarrow as pa\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "import pyarrow.dataset as ds\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "article_id = 14096681\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "out_dir = os.path.join(os.getcwd(), \"..\", \"data\", \"raw\", \"figshare\")\n",
    "file_to_download = \"data.zip\"\n",
    "\n",
    "# Get file url\n",
    "file_url = [\n",
    "    item_[\"download_url\"]\n",
    "    for item_ in requests.get(url, headers=headers).json()[\"files\"]\n",
    "    if item_[\"name\"] == file_to_download\n",
    "][0]\n",
    "\n",
    "# Check if file has already been downloaded\n",
    "if os.path.exists(os.path.join(out_dir, file_to_download)):\n",
    "    print(\"File already exists. Skipping.\")\n",
    "else:\n",
    "    print(f\"Writing file file {file_to_download} to directory {out_dir}\")\n",
    "\n",
    "    # Create an HTTP request\n",
    "    with requests.get(file_url, stream=True) as r:\n",
    "\n",
    "        # Check content length\n",
    "        content_length = int(r.headers.get(\"Content-Length\"))\n",
    "\n",
    "        # SDisplay progress bar\n",
    "        with tqdm.wrapattr(r.raw, \"read\", total=content_length, desc=\"\") as raw:\n",
    "\n",
    "            # Save file\n",
    "            os.makedirs(out_dir)\n",
    "            with open(os.path.join(out_dir, \n",
    "                                   file_to_download), \"wb\") as path:\n",
    "                shutil.copyfileobj(raw, path)\n",
    "\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "if not any(fname.endswith('.csv') for fname in os.listdir('.')):\n",
    "    # Unzip file with python\n",
    "    print(\"Unzipping file...\")\n",
    "    with zipfile.ZipFile(os.path.join(out_dir, file_to_download), \"r\") as zip_ref:\n",
    "        zip_ref.extractall(out_dir) # Extract all files to directory\n",
    "        zip_ref.close()\n",
    "    print(\"Unzipping complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combining data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_processed_dir = os.path.join(os.getcwd(), \"..\", \"data\", \"processed\", \"figshare\")\n",
    "file_to_exclude = \"observed_daily_rainfall_SYD.csv\"\n",
    "files = glob.glob(out_dir + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "\n",
    "# Combine data\n",
    "df = pd.concat(\n",
    "    (\n",
    "        pd.read_csv(file, index_col=0).assign(model=re.findall(r\"[^\\/]+(?=\\_daily)\", os.path.basename(file))[0])\n",
    "        for file in files\n",
    "        if file_to_exclude not in file\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write to file\n",
    "os.makedirs(out_processed_dir, exist_ok=True)  \n",
    "df.to_csv(os.path.join(out_processed_dir, \"processed_rainfall.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare run times on different machines - Combining data\n",
    "\n",
    "| Team Member        | Operating System | RAM  | Processor              | Is SSD | Time taken |\n",
    "|:------------------:|:----------------:|:----:|:----------------------:|:------:|:----------:|\n",
    "| Rakesh Pandey      | Ubuntu 20.04     | 32GB | Intel® Core™ i7-10870H | Yes    | 4min 51s   |\n",
    "| Mahsa Sarafrazi    | Windows 11 64-bit| 8 GB | Intel® Core™ i5-1035G4 |Yes     | 17min 4s   |\n",
    "| Gabe Fairbrother   |  Windows 10      | 32GB | Intel® Core™ i7-10875H | Yes    |     6min 40s       |\n",
    "| Michelle Wang      | Windows 10        | 16GB | Intel® Core™ i5-11300H  |      Yes                  |  15min 25s    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Load all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(out_processed_dir, \"processed_rainfall.csv\"), index_col=0)\n",
    "\n",
    "# Get the model counts\n",
    "print(\"Model counts:\")\n",
    "print(df.model.value_counts())\n",
    "\n",
    "# Describe the data\n",
    "print(\"Data description:\")  \n",
    "print(df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare run times on different machines - Load all columns\n",
    "\n",
    "| Team Member        | Operating System | RAM  | Processor              | Is SSD | Time taken |\n",
    "|:------------------:|:----------------:|:----:|:----------------------:|:------:|:----------:|\n",
    "| Rakesh Pandey      | Ubuntu 20.04     | 32GB | Intel® Core™ i7-10870H | Yes    | 1min 0s   |\n",
    "| Mahsa Sarafrazi    | Windows 11 64-bit   | 8 GB | Intel® Core™ i5-1035G4 | Yes    | 3min 37s  |\n",
    "| Gabe Fairbrother   |  Windows 10      | 32GB | Intel® Core™ i7-10875H | Yes    |   1min 18s       |\n",
    "| Michelle Wang      |  Windows 10        | 16GB | Intel® Core™ i5-11300H  |   Yes |  3min 29s          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Load only required columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "use_cols = [\"time\", \"rain (mm/day)\", \"model\"]\n",
    "df = pd.read_csv(\n",
    "    os.path.join(out_processed_dir, \"processed_rainfall.csv\"),\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    usecols=use_cols,\n",
    ")\n",
    "\n",
    "# Get the model counts\n",
    "print(\"Model counts:\")\n",
    "print(df.model.value_counts())\n",
    "\n",
    "# Describe the data\n",
    "print(\"Data description:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare run times on different machines - Load only required cols\n",
    "\n",
    "| Team Member        | Operating System | RAM  | Processor              | Is SSD | Time taken |\n",
    "|:------------------:|:----------------:|:----:|:----------------------:|:------:|:----------:|\n",
    "| Rakesh Pandey      | Ubuntu 20.04     | 32GB | Intel® Core™ i7-10870H | Yes    | 46.8s      |\n",
    "| Mahsa Sarafrazi    | Windows 64-bit   | 8 GB | Intel® Core™ i5-1035G4 | Yes    | 7min 34s  |\n",
    "| Gabe Fairbrother   |  Windows 10      | 32GB | Intel® Core™ i7-10875H | Yes    |    1min 26s      |\n",
    "| Michelle Wang      | Windows 10        | 16GB | Intel® Core™ i5-11300H  |   Yes |   3min 15s         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that time is now slightly reduced: loading required columns reduced time taken for most of us - 3 out of 4 members (previously from about 1 minutes+ to now under 1 minute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Change dtype and use only required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "\n",
    "use_cols = [\"time\", \"rain (mm/day)\", \"model\"]\n",
    "dtypes = {\"rain (mm/day)\": \"float32\", \"model\": \"str\"}\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(out_processed_dir, \"processed_rainfall.csv\"),\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    usecols=use_cols,\n",
    "    dtype=dtypes,\n",
    ")\n",
    "\n",
    "# Get the model counts\n",
    "print(\"Model counts:\")\n",
    "print(df.model.value_counts())\n",
    "\n",
    "# Describe the data\n",
    "print(\"Data description:\")\n",
    "print(df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare run times on different machines - Change dtype and load required cols\n",
    "\n",
    "| Team Member        | Operating System | RAM  | Processor              | Is SSD | Time taken |\n",
    "|:------------------:|:----------------:|:----:|:----------------------:|:------:|:----------:|\n",
    "| Rakesh Pandey      | Ubuntu 20.04     | 32GB | Intel® Core™ i7-10870H | Yes    | 46.1s      |\n",
    "| Mahsa Sarafrazi    | Windows 11 64-bit   | 8 GB | Intel® Core™ i5-1035G4 | Yes    | 9min 55s   |\n",
    "| Gabe Fairbrother   |  Windows 10      | 32GB | Intel® Core™ i7-10875H | Yes    |    1min 21s|\n",
    "| Michelle Wang      |  Windows 10        | 16GB | Intel® Core™ i5-11300H  |   Yes |   2min 58s         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adding onto the above, changing dtype has further reduced our time for most of us (3 out of 4 members) slightly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Use chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "\n",
    "df = pd.DataFrame()\n",
    "counts = pd.Series(dtype=int)\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    os.path.join(out_processed_dir, \"processed_rainfall.csv\"),\n",
    "    index_col=0,\n",
    "    parse_dates=True, \n",
    "    chunksize=1_000_000):\n",
    "    df = pd.concat([df, chunk])\n",
    "    \n",
    "\n",
    "# Get the model counts\n",
    "print(\"Model counts:\")\n",
    "print(df.model.value_counts())\n",
    "\n",
    "# Describe the data   \n",
    "print(\"Data description:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare run times on different machines - Chunking\n",
    "\n",
    "| Team Member        | Operating System | RAM  | Processor              | Is SSD | Time taken |\n",
    "|:------------------:|:----------------:|:----:|:----------------------:|:------:|:----------:|\n",
    "| Rakesh Pandey      | Ubuntu 20.04     | 32GB | Intel® Core™ i7-10870H | Yes    | 1min 34s   |\n",
    "| Mahsa Sarafrazi    | Windows 64-bit   | 8 GB | Intel® Core™ i5-1035G4 | Yes    | 5min 44s   | \n",
    "| Gabe Fairbrother   |  Windows 10      | 32GB | Intel® Core™ i7-10875H | Yes    |     2min 12s      |\n",
    "| Michelle Wang      |  Windows 10        | 16GB | Intel® Core™ i5-11300H  |   Yes  |    3min 50s        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For chunking, it seems like there is not much improvement for most of us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA Python Conclusion:**\n",
    "After trying out a few techinques, we can conclude that both loading the required columns and changing datatypes are effective ways of reducing runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "> The below class are some EDAs that are commented due to 2 main reasons:\n",
    "> 1. reading the whole data and working with pandas library, makes the computer to crash. Using a small part of the data and making conclusion is misleading, as it cannot give an accurate insight from the whole data.\n",
    "> 2. reading small part of data(1_000_000 rows) would increase the size of NB to 200MB which is cannot be uploaded on Github.\n",
    "> As a result, all EDA codes are available in below cells but we recommend not to run them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Plotting**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Reading the Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.read_csv(\n",
    "#     os.path.join(out_processed_dir, \"processed_rainfall.csv\"),\n",
    "#     index_col=0,\n",
    "#     usecols=[\"time\", \"rain (mm/day)\", \"model\"],\n",
    "#     parse_dates=True,\n",
    "# )\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Sampling 1_000_000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_sample = df.sample(n=1000000, random_state=42)\n",
    "# df_sample.to_csv(os.path.join(out_processed_dir, \"EDA.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Rainfall distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot = (\n",
    "#     alt.Chart(df_sample, title=\"Total rain distribution\")\n",
    "#     .mark_boxplot(extent=\"min-max\")\n",
    "#     .encode(alt.X(\"rain (mm/day)\"))\n",
    "# )\n",
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Rainfall distribution based on model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot = (\n",
    "#     alt.Chart(df_sample, title=\"Rain distribution based on model\")\n",
    "#     .mark_boxplot(extent=\"min-max\")\n",
    "#     .encode(\n",
    "#         alt.X(\"rain (mm/day)\"),\n",
    "#         alt.Y(\n",
    "#             \"model\",\n",
    "#             sort=alt.EncodingSortField(\n",
    "#                 field=\"rain (mm/day)\", op=\"median\", order=\"descending\"\n",
    "#             ),\n",
    "#         ),\n",
    "#         color=\"model\",\n",
    "#     )\n",
    "# )\n",
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Rainfall histogram based on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.data_transformers.disable_max_rows()\n",
    "# plot_hist = (\n",
    "#     alt.Chart(df_sample, title=\"Rain fall histogram based on model\")\n",
    "#     .mark_bar()\n",
    "#     .encode(alt.X(\"rain (mm/day)\"), alt.Y(\"count():Q\"), color=\"model\")\n",
    "#     .properties(width=180, height=180)\n",
    "#     .facet(facet=\"model\", columns=9)\n",
    "# )\n",
    "# plot_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall try out a few methods with simple EDA to test the efficiency of each method to convert data into R formats: Parquet file, Feather and Arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with only the model column\n",
    "\n",
    "out_processed_dir = os.path.join(os.getcwd(), \"..\", \"data\", \"processed\", \"figshare\")\n",
    "use_cols = [\"model\"]\n",
    "dtypes = {\"model\": \"str\"}\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(out_processed_dir, \"processed_rainfall.csv\"),\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    usecols=use_cols,\n",
    "    dtype=dtypes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Parquet file method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.exists(os.path.join(out_processed_dir, \"rainfall.parquet\")):\n",
    "    print(\"Parquet File already exists. Skipping.\")\n",
    "else:\n",
    "    df.to_parquet(os.path.join(out_processed_dir, \"rainfall.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "suppressMessages(library(arrow, warn.conflicts = FALSE))\n",
    "suppressMessages(library(dplyr, warn.conflicts = FALSE))\n",
    "library(here)\n",
    "\n",
    "ds <- open_dataset(here(\"data/processed/figshare/rainfall.parquet\"))\n",
    "result <- ds %>% count(model, sort=TRUE)\n",
    "\n",
    "# My windows comp crashes for this line! \n",
    "# print(result %>% collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To convert pandas df to parquet, it took 22.6s and then loading the 'ds' parquet file was super fast because it hasn't processed anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Feather method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create feather file from pandas df\n",
    "if os.path.exists(os.path.join(out_processed_dir, \"rainfall.feather\")):\n",
    "    print(\"Feather File already exists. Skipping.\")\n",
    "else:\n",
    "    df.reset_index().to_feather(os.path.join(out_processed_dir, \"rainfall.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "suppressMessages(library(arrow, warn.conflicts = FALSE))\n",
    "suppressMessages(library(dplyr, warn.conflicts = FALSE))\n",
    "library(here)\n",
    "\n",
    "# Can't seem to read this!\n",
    "# f_df <- read_feather(here(\"data/processed/figshare/rainfall.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Converting pandas df to feather file took 8.5s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### c) Arrow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This code crashes for my windows comp! Using the next cell instead.\n",
    "# %%time\n",
    "\n",
    "# dataset = ds.dataset(os.path.join(out_processed_dir, \"processed_rainfall.csv\"), format=\"csv\")\n",
    "# table = dataset.to_table()\n",
    "# r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rdf = pyra.converter.py2rpy(pa.Table.from_pandas(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R -i rdf\n",
    "library(dplyr)\n",
    "\n",
    "# Get the model counts\n",
    "result <- rdf %>% count(model, sort=TRUE)\n",
    "print(result %>% collect())\n",
    "\n",
    "# Describe the data\n",
    "print(\"Data description:\")\n",
    "print(summary(rdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Converting pandas df to arrow table object is fast: only 7.5s. Then printing the results of count by models and summary of dataset was only around 2.5s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final chosen approach: Arrow exchange**\n",
    "\n",
    "- After experimenting with different conversion methods to R, we concluded that the 'Arrow Exchange' method works best. \n",
    "- With parquet method, it took 26s just to convert the pandas file to parquet. And conversion to feather file format took 8.5s. The fastest was arrow which took around 7s. \n",
    "- The pyarrow package uses compiled code to efficiently convert a `pandas DataFrame` to an `Arrow` data structure, and the R arrow package can do the same from a `Arrow` data structure to a `R data.frame`.\n",
    "- The `arrow` table structure is also well-integrated with R's Dplyr package functionalities and makes EDA extremely fast and convenient, as exemplified in the code above where the printing of EDA results only took 2s.\n",
    "- Time spent on arrow's serialization/deserialization process is minimal and is also a zero-copy process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0431e97599bb9ae06b62e9e4cc0acb5e2f4237c339c3eea49a31a7ce8607a15c"
  },
  "kernelspec": {
   "display_name": "Python [conda env:525_2022]",
   "language": "python",
   "name": "conda-env-525_2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
